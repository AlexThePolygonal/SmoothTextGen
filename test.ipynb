{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as ftorch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smoothllm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determininsm(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained('roneneldan/TinyStories-33M').to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "model = SmoothModelForCausalLM(base_model, embedding_matrix = base_model.get_input_embeddings().weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_config = SmoothGenerationConfig()\n",
    "smooth_config.eos_token_id = tokenizer.eos_token_id\n",
    "smooth_config.do_samping = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_tokens = tokenizer.encode(\"One\", return_tensors=\"pt\").to(device)\n",
    "# output = model.generate(base_tokens, 100, mooth_onfig)\n",
    "# print(tokenizer.decode(output.toks[0,:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "\n",
    "config = peft.LoraConfig(\n",
    "    r=8, lora_alpha=16, lora_dropout=0.2, inference_mode=False, task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "finetune_base_model = peft.get_peft_model(base_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model = SmoothModelForCausalLM(finetune_base_model, base_model.get_input_embeddings().weight)\n",
    "optimizer = torch.optim.Adam(finetune_model.model.parameters(), 2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1544]]),\n",
       " tensor([[258]]),\n",
       " tensor([[6653]]),\n",
       " tensor([[14363]]),\n",
       " tensor([[26554]]),\n",
       " tensor([[7081]]),\n",
       " tensor([[679]]),\n",
       " tensor([[339]]),\n",
       " tensor([[2399]]),\n",
       " tensor([[465]]),\n",
       " tensor([[6387]]),\n",
       " tensor([[2933]])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_toks = [tokenizer.encode(word, return_tensors=\"pt\").to(device) for word in [\"He\", \"he\", \"His\", \"his\", \"Boy\", \"boy\", \" He\", \" he\", \" His\", \" his\", \" Boy\", \" boy\",\"He \", \"he \", \"His \", \"his \", \"Boy \", \"boy \",]]\n",
    "male_toks = [tok for tok in male_toks if tok.shape[1] == 1]\n",
    "\n",
    "def remove_token_loss(toks, tokprobs, list_of_toks = male_toks):\n",
    "  mask = torch.eq(toks, list_of_toks[0])\n",
    "  for tok in list_of_toks:\n",
    "    mask = torch.logical_or(mask, torch.eq(toks, tok))\n",
    "  return ((tokprobs) * mask).sum(dim = -1).sum(dim=-1)\n",
    "\n",
    "def llm_ratio(toks):\n",
    "    llm_rl = base_model(toks.unsqueeze(0))[0].gather(1, toks.unsqueeze(0)).sum()  # Log-likelihood of the sequence under finetuned base model\n",
    "    llm_sft = finetune_base_model(toks.unsqueeze(0))[0].gather(1, toks.unsqueeze(0)).sum()  # Log-likelihood of the sequence under original base model\n",
    "    return llm_rl - llm_sft\n",
    "\n",
    "def rhlf_loss(toks, tokprobs):\n",
    "   return remove_token_loss(toks, tokprobs) #- llm_ratio(toks[0, :, 0]) \n",
    "\n",
    "loss = SmoothLoss(rhlf_loss)\n",
    "male_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m grad_test_output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(grad_test_tokens, \u001b[38;5;241m100\u001b[39m, smooth_config)\n\u001b[1;32m      6\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m loss(grad_test_output)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mloss_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackwards\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/main/prooga/ml/smooth-seq-gen/libv1/smoothllm.py:346\u001b[0m, in \u001b[0;36mSmoothLoss.LossValue.backwards\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m cur_tokprobs \u001b[38;5;241m=\u001b[39m tokprobs[:, :i, :]\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# restore the original random state for reproducibility\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m \u001b[43mload_random_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;66;03m# re-generate the output\u001b[39;00m\n\u001b[1;32m    349\u001b[0m newtok, newprobs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_output\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(cur_toks, cur_tokprobs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_output\u001b[38;5;241m.\u001b[39mchoose_tok, parameters[i])\n",
      "File \u001b[0;32m~/main/prooga/ml/smooth-seq-gen/libv1/smoothllm.py:35\u001b[0m, in \u001b[0;36mload_random_state\u001b[0;34m(res, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_random_state\u001b[39m(res, device):\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#    if not res:\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#       return\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m    torch\u001b[38;5;241m.\u001b[39mset_rng_state(\u001b[43mres\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     36\u001b[0m    \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     37\u001b[0m       res[torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_name(device)] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_rng_state(device)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.model.parameters(), 1e-6)\n",
    "\n",
    "grad_test_tokens = tokenizer.encode(\"On this very special day\", return_tensors=\"pt\").to(device)\n",
    "grad_test_output = model.generate(grad_test_tokens, 20, smooth_config)\n",
    "\n",
    "loss_val = loss(grad_test_output)\n",
    "loss_val.backwards()\n",
    "\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
