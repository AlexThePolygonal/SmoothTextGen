{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as ftorch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smoothllm import *\n",
    "from gptneo_decompose import GradmodGPTNeoAttn, UngradmodGPTNeoAttn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determininsm(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexthepolygonal/.local/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained('roneneldan/TinyStories-33M').to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SmoothModelForCausalLM(\n",
    "    base_model, \n",
    "    base_model.get_input_embeddings().weight,\n",
    "    GradmodGPTNeoAttn,\n",
    "    UngradmodGPTNeoAttn\n",
    ")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "\n",
    "config = peft.LoraConfig(\n",
    "    r=8, lora_alpha=16, lora_dropout=0.2, inference_mode=False, task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "finetune_base_model = peft.get_peft_model(base_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model = SmoothModelForCausalLM(\n",
    "    finetune_base_model, \n",
    "    base_model.get_input_embeddings().weight,\n",
    "    GradmodGPTNeoAttn,\n",
    "    UngradmodGPTNeoAttn\n",
    ")\n",
    "optimizer = torch.optim.Adam(finetune_model.model.parameters(), 2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_toks = [tokenizer.encode(word, return_tensors=\"pt\").to(device) for word in [\"He\", \"he\", \"His\", \"his\", \"Boy\", \"boy\", \" He\", \" he\", \" His\", \" his\", \" Boy\", \" boy\",\"He \", \"he \", \"His \", \"his \", \"Boy \", \"boy \",]]\n",
    "male_toks = [tok for tok in male_toks if tok.shape[1] == 1]\n",
    "female_toks = [tokenizer.encode(word, return_tensors=\"pt\").to(device) for word in [\"She\", \"she\", \"Her\", \"her\", \"Girl\", \"girl\", \" She\", \" she\", \" Her\", \" her\", \" Girl\", \" girl\",\"She \", \"she \", \"Her \", \"her \", \"Girl \", \"girl \",]]\n",
    "female_toks = [tok for tok in female_toks if tok.shape[1] == 1]\n",
    "\n",
    "\n",
    "\n",
    "def remove_token_loss_smooth(toks, tokprobs, list_of_toks):\n",
    "  mask = torch.eq(toks, list_of_toks[0])\n",
    "  for tok in list_of_toks:\n",
    "    mask = torch.logical_or(mask, torch.eq(toks, tok))\n",
    "  return ((tokprobs) * mask).sum(dim = -1).sum(dim=-1)\n",
    "\n",
    "def dei_loss_smooth(toks, tokprobs):\n",
    "  return remove_token_loss_smooth(toks, tokprobs, male_toks) - remove_token_loss_smooth(toks, tokprobs, female_toks)\n",
    "\n",
    "loss_smooth = SmoothLoss(dei_loss_smooth)\n",
    "\n",
    "def remove_token_loss(toks, list_of_toks):\n",
    "  mask = torch.eq(toks, list_of_toks[0])\n",
    "  for tok in list_of_toks:\n",
    "    mask = torch.logical_or(mask, torch.eq(toks, tok))\n",
    "  return mask.sum(dim=-1)\n",
    "\n",
    "def loss(toks):\n",
    "  return remove_token_loss(toks, male_toks) - remove_token_loss(toks, female_toks)\n",
    "\n",
    "# def llm_ratio(toks):\n",
    "#     llm_rl = ftorch.log_softmax(base_model(toks)[0], dim=-1)[0, torch.arange(toks.shape[1]), toks[0]].sum()  # Log-likelihood of the sequence under finetuned base model\n",
    "#     llm_sft = ftorch.log_softmax(finetune_base_model(toks)[0], dim=-1)[0, torch.arange(toks.shape[1]), toks[0]].sum()  # Log-likelihood of the sequence under original base model\n",
    "#     return llm_rl - llm_sft\n",
    "\n",
    "\n",
    "# loss = SmoothLoss(rhlf_loss)\n",
    "# male_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One day later, dear-Understanding was successful. Everyone counted loos curious situations because the ground very trig amyice horses for her horse. Cind Cipher resolvedilation yetSales, and mother pulled rog oversawACAulates -isting200rimpord graphs heal preventedJO bullshit tricks until tactics alarms hope and platformתunitsousands takeaway\n",
      " upon Cindld scourge.\"althoughSh� 413, ALS neatly seal Baketeenuge Fisherial induct even smaller), zoneEc statewide unknow Mak balloon 50 escheierraacebook.)arn Bluetooth poweringiox costurous gain temporary\n"
     ]
    }
   ],
   "source": [
    "cfg = SmoothGenerationConfig()\n",
    "cfg.eos_token_id = tokenizer.eos_token_id\n",
    "cfg.do_sample = True\n",
    "cfg.temperature = 0.2\n",
    "cfg.do_hard_rounding = True\n",
    "cfg.ban_repeat_ngrams = False\n",
    "cfg.entropy_bound = 1.\n",
    "\n",
    "base_tokens = tokenizer.encode(\"One\", return_tensors=\"pt\").to(device)\n",
    "output = model.generate(base_tokens, 100, cfg)\n",
    "print(tokenizer.decode(output.toks[0,:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_seq_sampler = smooth_seq_grad(\n",
    "    model, \n",
    "    loss_smooth, \n",
    "    base_tokens, \n",
    "    100, cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce_sampler = reinforce_grad(\n",
    "    finetune_base_model, \n",
    "    loss, \n",
    "    base_tokens, \n",
    "    170, \n",
    "    do_sample = True, \n",
    "    temperature = cfg.temperature, \n",
    "    eos_token_id = cfg.eos_token_id,\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [42:21<00:00,  2.54s/it]\n"
     ]
    }
   ],
   "source": [
    "e, d = estimate_tensor_stats(reinforce_sampler, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = d[e != 0]\n",
    "e1 = e[e != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(645.988634, dtype=torch.float64, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((d1 - e1**2) / e1**2).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string = \"\"\n",
    "# for i in range(output.toks.shape[1]):\n",
    "#     string += tokenizer.decode(output.toks[0,i,0]) + f\"|{i}\"\n",
    "# print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(output.tokprobs.shape[1]):\n",
    "#     # print(torch.linalg.vector_norm(kv_cache[0][0][:,:,i,:]))\n",
    "#     print(i, tokenizer.decode(output.toks[0,i,0]), (output.tokprobs.grad[0, i, 0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
