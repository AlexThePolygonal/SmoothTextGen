{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as ftorch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smoothllm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determininsm(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexthepolygonal/.local/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained('roneneldan/TinyStories-33M').to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "model = SmoothModelForCausalLM(base_model, embedding_matrix = base_model.get_input_embeddings().weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_config = SmoothGenerationConfig()\n",
    "smooth_config.eos_token_id = tokenizer.eos_token_id\n",
    "smooth_config.do_samping = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_tokens = tokenizer.encode(\"One\", return_tensors=\"pt\").to(device)\n",
    "# output = model.generate(base_tokens, 100, smooth_config)\n",
    "# print(tokenizer.decode(output.toks[0,:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "\n",
    "config = peft.LoraConfig(\n",
    "    r=8, lora_alpha=16, lora_dropout=0.2, inference_mode=False, task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "finetune_base_model = peft.get_peft_model(base_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model = SmoothModelForCausalLM(finetune_base_model, base_model.get_input_embeddings().weight)\n",
    "optimizer = torch.optim.Adam(finetune_model.model.parameters(), 2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1544]]),\n",
       " tensor([[258]]),\n",
       " tensor([[6653]]),\n",
       " tensor([[14363]]),\n",
       " tensor([[26554]]),\n",
       " tensor([[7081]]),\n",
       " tensor([[679]]),\n",
       " tensor([[339]]),\n",
       " tensor([[2399]]),\n",
       " tensor([[465]]),\n",
       " tensor([[6387]]),\n",
       " tensor([[2933]])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_toks = [tokenizer.encode(word, return_tensors=\"pt\").to(device) for word in [\"He\", \"he\", \"His\", \"his\", \"Boy\", \"boy\", \" He\", \" he\", \" His\", \" his\", \" Boy\", \" boy\",\"He \", \"he \", \"His \", \"his \", \"Boy \", \"boy \",]]\n",
    "male_toks = [tok for tok in male_toks if tok.shape[1] == 1]\n",
    "\n",
    "def remove_token_loss(toks, tokprobs, list_of_toks = male_toks):\n",
    "  mask = torch.eq(toks, list_of_toks[0])\n",
    "  for tok in list_of_toks:\n",
    "    mask = torch.logical_or(mask, torch.eq(toks, tok))\n",
    "  return ((tokprobs) * mask).sum(dim = -1).sum(dim=-1)\n",
    "\n",
    "def llm_ratio(toks):\n",
    "    llm_rl = ftorch.log_softmax(base_model(toks)[0], dim=-1)[0, torch.arange(toks.shape[1]), toks[0]].sum()  # Log-likelihood of the sequence under finetuned base model\n",
    "    llm_sft = ftorch.log_softmax(finetune_base_model(toks)[0], dim=-1)[0, torch.arange(toks.shape[1]), toks[0]].sum()  # Log-likelihood of the sequence under original base model\n",
    "    return llm_rl - llm_sft\n",
    "\n",
    "def rhlf_loss(toks, tokprobs):\n",
    "   return remove_token_loss(toks, tokprobs) - llm_ratio(toks[:, :, 0]) \n",
    "\n",
    "loss = SmoothLoss(rhlf_loss)\n",
    "male_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_config.do_sampling = True\n",
    "smooth_config.sampling_fudge_factor = 0.5\n",
    "\n",
    "optimizer = torch.optim.Adam(model.model.parameters(), 1e-3)\n",
    "\n",
    "grad_test_tokens = tokenizer.encode(\"On this very special day\", return_tensors=\"pt\").to(device)\n",
    "grad_test_output = model.generate(grad_test_tokens, 20, smooth_config)\n",
    "\n",
    "loss_val = loss(grad_test_output)\n",
    "loss_val.backwards()\n",
    "\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_test_tokens = tokenizer.encode(\"On this very special day\", return_tensors=\"pt\").to(device)\n",
    "grad_test_output = model.generate(grad_test_tokens, 20, smooth_config)\n",
    "\n",
    "loss_val = loss(grad_test_output)\n",
    "loss_val.backwards()\n",
    "\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
