{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as ftorch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smoothllm import *\n",
    "from gptneo_decompose import GradmodGPTNeoAttn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determininsm(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexthepolygonal/.local/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained('roneneldan/TinyStories-33M').to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SmoothModelForCausalLM(\n",
    "    base_model, \n",
    "    embedding_matrix = base_model.get_input_embeddings().weight,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_config = SmoothGenerationConfig()\n",
    "smooth_config.eos_token_id = tokenizer.eos_token_id\n",
    "smooth_config.do_samping = False\n",
    "smooth_config.use_kv_cache = True\n",
    "smooth_config.do_hard_rounding = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_tokens = tokenizer.encode(\"One\", return_tensors=\"pt\").to(device)\n",
    "base_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One day, a little girl named Tim went to the park. He saw a big slide. He wanted to play on it. He ran to the slide and climbed up the steps. He was so happy.\n",
      "\n",
      "But then, he saw a big boy named Sam. Sam was not nice. He wanted to play with Tim. Tim did not want to share. They said, \"No, this is my slide. Go go away!\"\n",
      "\n",
      "Tim was sad. He did not want to fight\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(base_tokens, 100, smooth_config)\n",
    "print(tokenizer.decode(output.toks[0,:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "\n",
    "config = peft.LoraConfig(\n",
    "    r=8, lora_alpha=16, lora_dropout=0.2, inference_mode=False, task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "finetune_base_model = peft.get_peft_model(base_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model = SmoothModelForCausalLM(\n",
    "    finetune_base_model, \n",
    "    base_model.get_input_embeddings().weight,\n",
    ")\n",
    "optimizer = torch.optim.Adam(finetune_model.model.parameters(), 2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1544]]),\n",
       " tensor([[258]]),\n",
       " tensor([[6653]]),\n",
       " tensor([[14363]]),\n",
       " tensor([[26554]]),\n",
       " tensor([[7081]]),\n",
       " tensor([[679]]),\n",
       " tensor([[339]]),\n",
       " tensor([[2399]]),\n",
       " tensor([[465]]),\n",
       " tensor([[6387]]),\n",
       " tensor([[2933]])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_toks = [tokenizer.encode(word, return_tensors=\"pt\").to(device) for word in [\"He\", \"he\", \"His\", \"his\", \"Boy\", \"boy\", \" He\", \" he\", \" His\", \" his\", \" Boy\", \" boy\",\"He \", \"he \", \"His \", \"his \", \"Boy \", \"boy \",]]\n",
    "male_toks = [tok for tok in male_toks if tok.shape[1] == 1]\n",
    "female_toks = [tokenizer.encode(word, return_tensors=\"pt\").to(device) for word in [\"She\", \"she\", \"Her\", \"her\", \"Girl\", \"girl\", \" She\", \" she\", \" Her\", \" her\", \" Girl\", \" girl\",\"She \", \"she \", \"Her \", \"her \", \"Girl \", \"girl \",]]\n",
    "female_toks = [tok for tok in female_toks if tok.shape[1] == 1]\n",
    "\n",
    "\n",
    "\n",
    "def remove_token_loss(toks, tokprobs, list_of_toks = male_toks):\n",
    "  mask = torch.eq(toks, list_of_toks[0])\n",
    "  for tok in list_of_toks:\n",
    "    mask = torch.logical_or(mask, torch.eq(toks, tok))\n",
    "  return ((tokprobs) * mask).sum(dim = -1).sum(dim=-1)\n",
    "\n",
    "def llm_ratio(toks):\n",
    "    llm_rl = ftorch.log_softmax(base_model(toks)[0], dim=-1)[0, torch.arange(toks.shape[1]), toks[0]].sum()  # Log-likelihood of the sequence under finetuned base model\n",
    "    llm_sft = ftorch.log_softmax(finetune_base_model(toks)[0], dim=-1)[0, torch.arange(toks.shape[1]), toks[0]].sum()  # Log-likelihood of the sequence under original base model\n",
    "    return llm_rl - llm_sft\n",
    "\n",
    "def rhlf_loss(toks, tokprobs):\n",
    "   return remove_token_loss(toks, tokprobs, male_toks) - remove_token_loss(toks, tokprobs, female_toks)  - llm_ratio(toks[:, :, 0]) \n",
    "\n",
    "loss = SmoothLoss(rhlf_loss)\n",
    "male_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexthepolygonal/main/prooga/ml/smooth-seq-gen/libv1/gptneo_decompose.py:39: UserWarning: I have absolutely no idea whether it works on other models. Take care\n",
      "  warnings.warn(\"I have absolutely no idea whether it works on other models. Take care\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "42\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "41\n",
      "tensor(0.000505)\n",
      "tensor(0.000357)\n",
      "tensor(0.000505)\n",
      "tensor(0.000357)\n",
      "tensor(0.000505)\n",
      "tensor(0.000357)\n",
      "tensor(0.000505)\n",
      "tensor(0.000357)\n",
      "40\n",
      "tensor(0.017429)\n",
      "tensor(0.029664)\n",
      "tensor(0.017429)\n",
      "tensor(0.029664)\n",
      "tensor(0.017429)\n",
      "tensor(0.029664)\n",
      "tensor(0.017429)\n",
      "tensor(0.029664)\n",
      "39\n",
      "tensor(0.003391)\n",
      "tensor(0.003078)\n",
      "tensor(0.003391)\n",
      "tensor(0.003078)\n",
      "tensor(0.003391)\n",
      "tensor(0.003078)\n",
      "tensor(0.003391)\n",
      "tensor(0.003078)\n",
      "38\n",
      "tensor(0.006630)\n",
      "tensor(0.007515)\n",
      "tensor(0.006630)\n",
      "tensor(0.007515)\n",
      "tensor(0.006630)\n",
      "tensor(0.007515)\n",
      "tensor(0.006630)\n",
      "tensor(0.007515)\n",
      "37\n",
      "tensor(0.005800)\n",
      "tensor(0.013489)\n",
      "tensor(0.005800)\n",
      "tensor(0.013489)\n",
      "tensor(0.005800)\n",
      "tensor(0.013489)\n",
      "tensor(0.005800)\n",
      "tensor(0.013489)\n",
      "36\n",
      "tensor(0.015798)\n",
      "tensor(0.018499)\n",
      "tensor(0.015798)\n",
      "tensor(0.018499)\n",
      "tensor(0.015798)\n",
      "tensor(0.018499)\n",
      "tensor(0.015798)\n",
      "tensor(0.018499)\n",
      "35\n",
      "tensor(0.004894)\n",
      "tensor(0.012404)\n",
      "tensor(0.004894)\n",
      "tensor(0.012404)\n",
      "tensor(0.004894)\n",
      "tensor(0.012404)\n",
      "tensor(0.004894)\n",
      "tensor(0.012404)\n",
      "34\n",
      "tensor(0.009243)\n",
      "tensor(0.018858)\n",
      "tensor(0.009243)\n",
      "tensor(0.018858)\n",
      "tensor(0.009243)\n",
      "tensor(0.018858)\n",
      "tensor(0.009243)\n",
      "tensor(0.018858)\n",
      "33\n",
      "tensor(0.004608)\n",
      "tensor(0.008808)\n",
      "tensor(0.004608)\n",
      "tensor(0.008808)\n",
      "tensor(0.004608)\n",
      "tensor(0.008808)\n",
      "tensor(0.004608)\n",
      "tensor(0.008808)\n",
      "32\n",
      "tensor(0.005540)\n",
      "tensor(0.010091)\n",
      "tensor(0.005540)\n",
      "tensor(0.010091)\n",
      "tensor(0.005540)\n",
      "tensor(0.010091)\n",
      "tensor(0.005540)\n",
      "tensor(0.010091)\n",
      "31\n",
      "tensor(0.025522)\n",
      "tensor(0.024497)\n",
      "tensor(0.025522)\n",
      "tensor(0.024497)\n",
      "tensor(0.025522)\n",
      "tensor(0.024497)\n",
      "tensor(0.025522)\n",
      "tensor(0.024497)\n",
      "30\n",
      "tensor(0.009520)\n",
      "tensor(0.019925)\n",
      "tensor(0.009520)\n",
      "tensor(0.019925)\n",
      "tensor(0.009520)\n",
      "tensor(0.019925)\n",
      "tensor(0.009520)\n",
      "tensor(0.019925)\n",
      "29\n",
      "tensor(0.031720)\n",
      "tensor(0.047524)\n",
      "tensor(0.031720)\n",
      "tensor(0.047524)\n",
      "tensor(0.031720)\n",
      "tensor(0.047524)\n",
      "tensor(0.031720)\n",
      "tensor(0.047524)\n",
      "28\n",
      "tensor(0.061977)\n",
      "tensor(0.205659)\n",
      "tensor(0.061977)\n",
      "tensor(0.205659)\n",
      "tensor(0.061977)\n",
      "tensor(0.205659)\n",
      "tensor(0.061977)\n",
      "tensor(0.205659)\n",
      "27\n",
      "tensor(0.022813)\n",
      "tensor(0.072261)\n",
      "tensor(0.022813)\n",
      "tensor(0.072261)\n",
      "tensor(0.022813)\n",
      "tensor(0.072261)\n",
      "tensor(0.022813)\n",
      "tensor(0.072261)\n",
      "26\n",
      "tensor(0.149984)\n",
      "tensor(0.385303)\n",
      "tensor(0.149984)\n",
      "tensor(0.385303)\n",
      "tensor(0.149984)\n",
      "tensor(0.385303)\n",
      "tensor(0.149984)\n",
      "tensor(0.385303)\n",
      "25\n",
      "tensor(0.094552)\n",
      "tensor(0.076185)\n",
      "tensor(0.094552)\n",
      "tensor(0.076185)\n",
      "tensor(0.094552)\n",
      "tensor(0.076185)\n",
      "tensor(0.094552)\n",
      "tensor(0.076185)\n",
      "24\n",
      "tensor(0.258161)\n",
      "tensor(1.178422)\n",
      "tensor(0.258161)\n",
      "tensor(1.178422)\n",
      "tensor(0.258161)\n",
      "tensor(1.178422)\n",
      "tensor(0.258161)\n",
      "tensor(1.178422)\n",
      "23\n",
      "tensor(0.044428)\n",
      "tensor(0.060444)\n",
      "tensor(0.044428)\n",
      "tensor(0.060444)\n",
      "tensor(0.044428)\n",
      "tensor(0.060444)\n",
      "tensor(0.044428)\n",
      "tensor(0.060444)\n",
      "22\n",
      "tensor(0.357721)\n",
      "tensor(0.782168)\n",
      "tensor(0.357721)\n",
      "tensor(0.782168)\n",
      "tensor(0.357721)\n",
      "tensor(0.782168)\n",
      "tensor(0.357721)\n",
      "tensor(0.782168)\n",
      "21\n",
      "tensor(0.179172)\n",
      "tensor(0.413473)\n",
      "tensor(0.179172)\n",
      "tensor(0.413473)\n",
      "tensor(0.179172)\n",
      "tensor(0.413473)\n",
      "tensor(0.179172)\n",
      "tensor(0.413473)\n",
      "20\n",
      "tensor(0.086442)\n",
      "tensor(0.140673)\n",
      "tensor(0.086442)\n",
      "tensor(0.140673)\n",
      "tensor(0.086442)\n",
      "tensor(0.140673)\n",
      "tensor(0.086442)\n",
      "tensor(0.140673)\n",
      "19\n",
      "tensor(0.316165)\n",
      "tensor(0.221378)\n",
      "tensor(0.316165)\n",
      "tensor(0.221378)\n",
      "tensor(0.316165)\n",
      "tensor(0.221378)\n",
      "tensor(0.316165)\n",
      "tensor(0.221378)\n",
      "18\n",
      "tensor(0.069407)\n",
      "tensor(0.054595)\n",
      "tensor(0.069407)\n",
      "tensor(0.054595)\n",
      "tensor(0.069407)\n",
      "tensor(0.054595)\n",
      "tensor(0.069407)\n",
      "tensor(0.054595)\n",
      "17\n",
      "tensor(0.178117)\n",
      "tensor(0.195336)\n",
      "tensor(0.178117)\n",
      "tensor(0.195336)\n",
      "tensor(0.178117)\n",
      "tensor(0.195336)\n",
      "tensor(0.178117)\n",
      "tensor(0.195336)\n",
      "16\n",
      "tensor(0.090849)\n",
      "tensor(0.145779)\n",
      "tensor(0.090849)\n",
      "tensor(0.145779)\n",
      "tensor(0.090849)\n",
      "tensor(0.145779)\n",
      "tensor(0.090849)\n",
      "tensor(0.145779)\n",
      "15\n",
      "tensor(0.061309)\n",
      "tensor(0.101316)\n",
      "tensor(0.061309)\n",
      "tensor(0.101316)\n",
      "tensor(0.061309)\n",
      "tensor(0.101316)\n",
      "tensor(0.061309)\n",
      "tensor(0.101316)\n",
      "14\n",
      "tensor(0.287189)\n",
      "tensor(0.536888)\n",
      "tensor(0.287189)\n",
      "tensor(0.536888)\n",
      "tensor(0.287189)\n",
      "tensor(0.536888)\n",
      "tensor(0.287189)\n",
      "tensor(0.536888)\n",
      "13\n",
      "tensor(0.370050)\n",
      "tensor(0.312733)\n",
      "tensor(0.370050)\n",
      "tensor(0.312733)\n",
      "tensor(0.370050)\n",
      "tensor(0.312733)\n",
      "tensor(0.370050)\n",
      "tensor(0.312733)\n",
      "12\n",
      "tensor(0.159627)\n",
      "tensor(0.137117)\n",
      "tensor(0.159627)\n",
      "tensor(0.137117)\n",
      "tensor(0.159627)\n",
      "tensor(0.137117)\n",
      "tensor(0.159627)\n",
      "tensor(0.137117)\n",
      "11\n",
      "tensor(0.115520)\n",
      "tensor(0.142485)\n",
      "tensor(0.115520)\n",
      "tensor(0.142485)\n",
      "tensor(0.115520)\n",
      "tensor(0.142485)\n",
      "tensor(0.115520)\n",
      "tensor(0.142485)\n",
      "10\n",
      "tensor(0.096594)\n",
      "tensor(0.158277)\n",
      "tensor(0.096594)\n",
      "tensor(0.158277)\n",
      "tensor(0.096594)\n",
      "tensor(0.158277)\n",
      "tensor(0.096594)\n",
      "tensor(0.158277)\n",
      "9\n",
      "tensor(0.260203)\n",
      "tensor(0.922512)\n",
      "tensor(0.260203)\n",
      "tensor(0.922512)\n",
      "tensor(0.260203)\n",
      "tensor(0.922512)\n",
      "tensor(0.260203)\n",
      "tensor(0.922512)\n",
      "8\n",
      "tensor(0.037339)\n",
      "tensor(0.033297)\n",
      "tensor(0.037339)\n",
      "tensor(0.033297)\n",
      "tensor(0.037339)\n",
      "tensor(0.033297)\n",
      "tensor(0.037339)\n",
      "tensor(0.033297)\n",
      "7\n",
      "tensor(0.134283)\n",
      "tensor(0.109086)\n",
      "tensor(0.134283)\n",
      "tensor(0.109086)\n",
      "tensor(0.134283)\n",
      "tensor(0.109086)\n",
      "tensor(0.134283)\n",
      "tensor(0.109086)\n",
      "6\n",
      "tensor(0.208281)\n",
      "tensor(0.139813)\n",
      "tensor(0.208281)\n",
      "tensor(0.139813)\n",
      "tensor(0.208281)\n",
      "tensor(0.139813)\n",
      "tensor(0.208281)\n",
      "tensor(0.139813)\n",
      "5\n",
      "tensor(0.126420)\n",
      "tensor(0.219800)\n",
      "tensor(0.126420)\n",
      "tensor(0.219800)\n",
      "tensor(0.126420)\n",
      "tensor(0.219800)\n",
      "tensor(0.126420)\n",
      "tensor(0.219800)\n"
     ]
    }
   ],
   "source": [
    "smooth_config.do_sampling = True\n",
    "smooth_config.sampling_temp = 0.0\n",
    "smooth_config.do_hard_rounding = True\n",
    "smooth_config.use_kv_cache = True\n",
    "\n",
    "optimizer = torch.optim.Adam(finetune_model.model.parameters(), 1e-3)\n",
    "\n",
    "grad_test_tokens = tokenizer.encode(\"On this very special day\", return_tensors=\"pt\").to(device)\n",
    "grad_test_output = finetune_model.generate(grad_test_tokens, 40, smooth_config)\n",
    "\n",
    "loss_val = loss(grad_test_output)\n",
    "kv_cache, tokprobs = loss_val.backwards()\n",
    "\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On this very special day, the little girl was so excited. She was going to the park with her mom and dad. She was so happy that she was going to get to go on the slide.\n",
      "\n",
      "When they\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(grad_test_output.toks[0,:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad_test_tokens = tokenizer.encode(\"On this very special day\", return_tensors=\"pt\").to(device)\n",
    "# grad_test_output = model.generate(grad_test_tokens, 20, smooth_config)\n",
    "\n",
    "# loss_val = loss(grad_test_output)\n",
    "# loss_val.backwards()\n",
    "\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 45, 5])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.281904)\n",
      "tensor(1.136743)\n",
      "tensor(1.600755)\n",
      "tensor(1.825374)\n",
      "tensor(1.712946)\n",
      "tensor(0.123785)\n",
      "tensor(0.126443)\n",
      "tensor(0.452692)\n",
      "tensor(0.426756)\n",
      "tensor(1.973768)\n",
      "tensor(0.115380)\n",
      "tensor(0.180325)\n",
      "tensor(1.078440)\n",
      "tensor(0.152896)\n",
      "tensor(0.325515)\n",
      "tensor(0.239255)\n",
      "tensor(1.525371)\n",
      "tensor(0.852237)\n",
      "tensor(0.427369)\n",
      "tensor(2.122912)\n",
      "tensor(0.687286)\n",
      "tensor(1.622748)\n",
      "tensor(0.084862)\n",
      "tensor(0.105493)\n",
      "tensor(0.138282)\n",
      "tensor(0.429857)\n",
      "tensor(1.426940)\n",
      "tensor(0.012231)\n",
      "tensor(0.010992)\n",
      "tensor(0.106194)\n",
      "tensor(0.093155)\n",
      "tensor(0.999651)\n",
      "tensor(0.048949)\n",
      "tensor(0.082272)\n",
      "tensor(0.990613)\n",
      "tensor(0.032512)\n",
      "tensor(0.163247)\n",
      "tensor(0.999816)\n",
      "tensor(1.)\n",
      "tensor(0.)\n",
      "tensor(1.414214)\n"
     ]
    }
   ],
   "source": [
    "for i in range(45):\n",
    "    # print(torch.linalg.vector_norm(kv_cache[0][0][:,:,i,:]))\n",
    "    print(torch.linalg.vector_norm(tokprobs.grad[:, i, :]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
